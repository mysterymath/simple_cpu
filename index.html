<!doctype html>
<html lang="en-US">
<title>A Simple CPU</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>
    html {
      background-color: #002b36;
    }
    .content {
      color: #839496;
      font-family: Georgia, serif;
      line-height: 1.5;
      max-width: 60ch;
      margin-left: auto;
      margin-right: auto;
    }
    h1,h2,h3,h4,h5,h6 {
      color: #93a1a1;
      font-family: sans-serif;
      line-height: normal;
    }
    h1 {
      text-align: center;
    }
    h2 {
      font-size: 1.2em;
    }
    p,ul,ol {
      break-inside: avoid;
    }
    a:link {
      color: #268bd2;
    }
    a:visited {
      color: #d33682;
    }
    code {
      font-size: large;
    }
    .not {
      text-decoration: overline;
    }
    .video {
      max-width: 100%;
      /* This is relative to the width, preserving the aspect ratio. */
      padding-bottom: 56.25%;
      position: relative;
      width: 560px;
    }
    iframe {
      height: 100%;
      left: 0;
      position: absolute;
      top: 0;
      width: 100%;
    }
</style>
<div class="content">
<h1>A Simple CPU</h1>
<h2>Magic</h2>
<p>When I was a child, I marvelled at our family computer. Occasionally, when I
was bored of shooting things in Duke Nukem 3D or Doom, I'd lean my face in
really close to the monitor and start nudging the arrow keys. As the game
camera moved about the virtual space, the pixellated rectangles on screen would
shift and distort in complex patterns. I was convinced that if I just stared at
it long enough, I'd figure out the trick they used to make the colors on screen
change to look like a 3D room.

<p>In high school, I finally learned enough math to figure it out. I even built
a little ray-casting 3D engine. (Although I later learned neither game actually
used ray-casting. Que sera.) It was one thing to work out on paper how to use
wall distances to shrink or expand a pixel-wide column of texture; it was
another thing to walk around a room made of your own math.

<p>In my childhood, I'd been shown a magic trick. Hocus pocus, now you see a
room! Later, I'd learned the trick myself.

<p>In college, I learned of computer architecture, compilers, network stacks,
algorithms and all the trappings of software. I took a digital circuit design
class and made sequential logic circuits on breadboards. A deeper magicks had
been revealed: computing itself.

<p>I learned how these tricks were done, but I couldn't really do them myself.
I wanted to make a CPU to embody what I'd learned, but every path towards this
goal felt either trivial or impossible.  I could hand wire relays or
transistors into flip flops and half adders, but that would take forever. I
could program it all into a FPGA; too much like programming. I could hook
together a bunch of pre-built ALUs and registers, but that felt like assembling
LEGOs. So I put the project on the shelf for a long time.

<h2>Simplicity</h2>

<p>The project popped back into my mind a while ago with a twist. Even toy CPUs
usually have instruction decoding, data path switching, an ALU, etc. But they
are simplified models of real CPUs, and real CPUs need to be efficient,
performant, and practical.

<p>My CPU wouldn't need to be any of those things; I'd just want it to
compute the Fibonacci sequence in under a day or so. This opened up an
enormous new space of hilariously impractical, but maybe simpler, designs.

<p>Any <a href="https://en.wikipedia.org/wiki/Turing_completeness">Turing-complete</a>
computer architecture can be reduced to any other via software rewriting. So
long as the transformation wouldn't make a Fibonacci program completely
infeasibile, any such architecture would work just fine. I also didn't want to
do a lot of wiring, so I was also looking for one that could be built from very
few electronic parts.

<h2>One instruction to rule them all</h2>

<p>Academics and nerds have invented a dazzling variety of Turing-complete
abstract machines with only one type of instruction. These are appropriately
named <a href="https://en.wikipedia.org/wiki/One_instruction_set_computer">one
instruction set computers</a> (OISCs). With only one instruction type, I could
skip instruction decoding and switching entirely.

<p><a href="https://en.wikipedia.org/wiki/One_instruction_set_computer#Subtract_and_branch_if_less_than_or_equal_to_zero">SUBLEQ</a>
(subtract and branch if result &lt;= 0) is probably the most famous OISC.
I explored it thoroughly, but implementing it seemed hardly any easier than a
normal CPU. SUBLEQ requires an ALU for subtraction with a status bit for the
comparison, and switched behavior based on the status bit for the conditional
branch.

<p>I consulted
<a href="https://en.wikipedia.org/wiki/One_instruction_set_computer">various</a>
<a href="https://esolangs.org/wiki/OISC">lists</a> of OISCs to find the perfect
one: <a href="https://esolangs.org/wiki/ByteByteJump">byte-byte-jump</a>. Any
useful CPU has to to accept input (read) and deliver output (write).
Byte-byte-jump doesn't require anything extra; just read and write.

<p>Byte-byte-jump takes three addresses as arguments: source, destination, and
jump. It reads a byte from the source address, writes it to the destination
address, and changes the instruction pointer to the jump address. If a CPU can
do this, then it can do anything.

<h2>Can't add, doesn't even try</h2>

<p>It might seem magical that such a simple operation could implement
arithmetic, comparison, and conditional branching. As with all magic tricks,
it's a bit less impressive once you know how.

<p><a href="https://en.wikipedia.org/wiki/Lookup_table">Tables</a>.  A program
can perform any arithmetic operation by including a boring table mapping all
possible inputs to the corresponding output. With 8-bit operations, unary
operations are 256-byte tables, and binary operations are two-dimensional 64
KiB tables.

<p>Real programs use lookup tables for DES S-boxes, hash functions, etc.  This
is no different, except that usually lookup tables make a program more
efficient; here they make a program <i>possible</i>.

<p>The program modifies itself at runtime to perform table lookups.
Specifically, it overwrites the low byte of a byte-byte-jump instruction with
the table index. The high byte stays fixed as the table offset. Once the
modified instruction runs, its source address is the correct location inside
the table.  Conditional branching involves overwriting the jump address
instead.

<p>I didn't want to actually include a bunch of 64 KiB tables in my program,
though. When I start measuring things in megabytes, the nostalgia goggles fall
right off. No matter. 4-bit operations chained together can do any 8-bit
operation, and 4-bit binary operations only use 256 bytes.

<p>The cheapest configuration of the <a
href="https://en.wikipedia.org/wiki/IBM_1620">IBM 1620</a> replaced expensive
logic with arithmetic tables in cheaper program storage (punched cards and the
like).  As a result, users took it's internal code name, CADET, to stand for
"Can't Add, Doesn't Even Try."

<h2>Thought</h2>

<p>With software out of the way, let's talk about hardware. Generally, simple
main memories can only read or write one address at a time. Byte-byte-jump
involves two reads and one write. In between, the CPU will need to remember
some data.

<p>Specifically, the CPU must remember the source and destination addresses,
the instruction pointer, and the byte to be written. The CPU communicates these
with main memory at different times, so the CPU needs some way to read or write
each in isolation.

<p>There are two easy ways to differentiate things in hardware: space and time.
Parallel systems include multiple copies of a thing with different wires and
pins for each. Serial systems use the same pins and wires to handle different
items at different times, with the timing coordinated by some protocol. Using
serial whenever possible should produce the smallest system.

<p>I couldn't find any cheap serial register chips, so I started shopping for
serial SRAMs. I narrowed this down to the 64KiB
<a href="https://www.microchip.com/wwwproducts/en/23LC512">Microchip 23LC512</a>.
This chip uses
<a href="https://en.wikipedia.org/wiki/Serial_Peripheral_Interface">SPI</a>
to communicate data, addresses, and read/write operations with the chip. SPI is
the simplest serial communications protocol I can imagine, and the chip only
costs around a dollar.

<p>I'll revel for a moment in the absurdity of using 64KiB of RAM to remember
48 bits. This one chip has more switching elements than the first ten of
mankinds' computers combined. Yet, the smallest SPI SRAM I could find was still
8KiB, and the 64KiB one was the smallest with a sequential mode I desparately
needed.

<p>The SRAM also has an instruction set, address pointer, operating modes,
registers, and a communications protocol. I wondered, did I just buy a
dumbed-down CPU to make a CPU? Sort of, but not really. The SRAM doesn't have
an instruction pointer or control flow. It cannot request instructions for
itself; the outside system must spoonfeed them in.

<p>It's just an SRAM, albeit a very fancy one. The fanciness makes it that
much easier to teach it how to be a CPU.

<h2>Instinct</h2>

<p>The SRAM has eight pins, but only three are interesting: chip select (CS),
serial input (SI), and serial output (SO). It can't actually input and output
at the same time, so I wired SI and SO together into one "SIO" pin. The
instruction (read/write), address and data (both in and out) are sent over SIO
once CS is activated; the operation ends once CS is deactivated.

<p>The main memory similarly needs communicate lines with the CPU. For this, I
basically just copied and tweaked the SRAM's. The main memory has its own CS
line, but it shares SIO with the SRAM, allowing bidirectional communications
between the two.

<p>Together, some sequence of 1's and 0's on SRAM CS, main memory CS, and SIO
make SRAM perform byte-byte-jump. This is not an obvious statement; deriving
the 1's and 0's involved quit a lot of work; a few months of fiddling in my
off-hours. The derivation is intricate but uninteresting.

<p>The result for the two CS lines is fairly straightforward. The sequence of
1's and 0's needed to execute byte-byte-jump are always fixed and can be
repeated indefinitely. The SIO line is different. Some times the
byte-byte-jump's "instincts" need to kick in to give the SRAM or main memory
instructions or addresses. Other times, the SRAM needs to drive the line; still
others, main memory.

<p>SIO has three control states: 0, 1, and "Z".  "Z" means "shut up and let the
SRAM or main memory speak." We can take Z to itself be a HOLD sequence of 1's
and 0's; whenever HOLD is 1, SIO is driven by the SRAM or main memory; whenever
HOLD is 0, SIO is driven to either 0 or 1, depending on the SIO sequence.

<p>So, how to drive these pins... how indeed. More SRAMs, naturally! Take an
SRAM and fill it with as many copies of control signal as will fit. Issue a
read to address zero, and the SRAM clocks out the signal until it reaches the
end of its capacity. At this point, it wraps back around to zero and continues
reading.

<p>This covers the two CS lines, but we still need the HOLD mechanism for SIO.
Luckily, Microship SRAMs have a HOLD pin that sets their output to Z. This pin
on the SIO control SRAM be driven by another SRAM, for a total of four control
SRAMs and one data SRAM.

<p>While I could have built this using five identical SRAMs, programming each
of them with control signals on each boot filled me with mild dread. So
instead, I used four 1024-bit SPI
<a href="http://www.microchip.com/wwwproducts/25LC010A">EEPROMs</a>,
which have nearly identical interfaces. Once programmed, the EEPROMs retain
their control behavior indefinitely even without power.

<h2>Implementation</h2>

<p>This puts the total bill of materials at:

<ul>
  <li>Microchip SRAM
  <li>4 Microchip EEPROMs
  <li>2 pullup resistors (for hold and system chip select)
  <li>4 LEDs w/ resistors, to provide blinkenlights for each signal
  <li>40 or so wires
</ul>

<p>I bought all this from Mouser and waited for it to arrive. It cost me about
$14, and $7 of that was shipping.

<p>The part of main memory was to be played by a Raspberry PI and its GPIO
pins.  I wrote a RPi.GPIO program to accept read and write operations over the
SIO and main memory CS lines, using a 64 KiB Python array array as backing
storage. I special-cased writes to 0xFFFF to print to the terminal.

<p>I wrote a script to program each of the EEPROMs with its control signal,
then verify that the EEPROM would repeatedly output that signal if a zero read
were issued. I did this for each EEPROM chip in turn. Finally, I wired them
together with each other, the SRAM, and the Raspberry PI.

<p>I turned it on for the first time. It immediately started executing
byte-byte-jumps, instead the expected behavior, catching fire. All things
considered, it took around 200 clock cycles to execute a single byte-byte-jump.

<p>I incrementally built up a library of macros using Python to program a
Fibonacci generating program into the memory array. All things considered,
it took around 200 byte-byte-jump instructions to execute an 8-bit addition.

<p>You can see it in action at various clock rates below. The second-rightmost
LED is SIO, and all of the others are control lines. The Turing magic happens
on the SIO LED; the others are perfectly cyclical. Thought and instinct.

<div class="video">
  <iframe
    src="https://www.youtube-nocookie.com/embed/videoseries?list=PL4Aym2GNCg4hb7xvSkjSgP3m2xWIY_TZK"
    frameborder="0"
    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>
</div>

<h2>Discussion</h2>

<p>Compared to the time it took to conceive, design, and program this CPU,
building it was incredibly straightforward. It operated reliably at all the
clock rates I could get Python to generate. Admittedly, that's not very fast,
but I got the thing running at 80kHz, and it's only takes around 40,000 cycles
to do an addition. That is, it can do around 2 additions per second, which is
more than I can do, and I bet it can do them all night!

<p>The final design resembles early computers far more than modern ones. All
early computers were bit-serial, and very limited instruction sets supplemented
with lookup tables were common.  When you think about it, that's not too
surprising, since in both cases, simple implementation is king.  I wanted it to
be simple because I'm lazy; but they needed it to be simple, since technology
placed tight limitations on what they could build, even with the backing of
major world governments and universities.

<p>I can safely say that running this CPU for the first time was exactly as
satisfying as I'd hoped. I still can't believe the damn thing actually
<b>computes</b>, and faster than I can! Given how many things could have broken
along the way, the fact that it does what I wanted is, well, pretty magical.

<p>I left out a tremendous amount of detail about how the CPU works, because it
was extremely boring. Still, if anyone wants to know the dirty details, shoot
me an email.

<address>Daniel Thornburgh (<a href="mailto:mysterymath@gmail.com">mysterymath@gmail.com</a>)</address>
