<!doctype html>
<html lang="en-US">
<title>A Simple CPU</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>
    html {
        background-color: #002b36;
    }
    .content {
        color: #839496;
	      font-family: Georgia, serif;
        line-height: 1.5;
        max-width: 60ch;
        margin-left: auto;
        margin-right: auto;
    }
    h1,h2,h3,h4,h5,h6 {
        color: #93a1a1;
	      font-family: sans-serif;
        line-height: normal;
    }
    p,ul,ol {
        break-inside: avoid;
    }
    a:link {
      color: #268bd2;
    }
    a:visited {
      color: #d33682;
    }
    code {
      font-size: large;
    }
    .not {
      text-decoration: overline;
    }
</style>
<div class="content">
<h1>A Simple CPU</h1>
<h2>Magic</h2>
<p>When I was a child, I marvelled at our family computer. I played a lot of
Duke Nukem 3D and Doom, and every once in a while, I'd get bored of shooting
things. Invariably, I'd lean my face in real close the the monitor and start
nudging the arrow keys. As the game camera moved through the virtual space,
the pixellated rectangles on screen would distort in complex patterns. I was
convinced that if I just stared at it long enough, I'd figure out the trick
they used to make the colors on screen change to look like a 3D room.

<p>In high school, I finally did learn enough math to figure that one out. I
even built a little ray-casting 3D engine to boot. (Ironically, neither of
the above games used ray-casting.) It was one thing to have figured out on
paper how the wall distances could be used to shrink or expand a 1-pixel
column of texture; it was another thing entirely to see a room made of your
own math on screen.

<p>I felt that in my childhood, I'd been shown a magic trick. Hocus pocus,
now you see a room! Then, as an adult, I could perform the trick myself.

<p>In college, I learned of computer architecture and compilers and network
stacks and algorithms and all the trappings of software. I even took a
digital circuit design class and made some nice sequential logic circuits. A
deeper magicks had been revealed; I learned some core tricks behind computing
itself.

<p>I understood how the trick was done, but I couldn't do it myself. I wanted
to make a CPU to play with what I'd learned, but every idea I had for a
project was either too easy or too hard. I could hand wire relays or
transistors into flip flops and half adders, but that would take forever. I
could program it all into a FPGA, but that felt way too easy. I could hook
together a bunch of pre-built ALUs and registers and the like, but that
didn't feel satisfying either. I put the project on the shelf.

<h2>Simplicity</h2>

<p>A ways back, the project popped back into my mind, but it had taken on a
bizarre form. Sans using a FPGA, all of the ways of wiring up a CPU are
pretty complicated. You've got an ALU to build, data paths to wire,
instruction decoding, etc. There's just a lot of stuff. But how much of that
is actually necessary?

<p>By definition, any Turing-equivalent computer architecture can be reduced
to any other via a software transformation. However, such a reduction can
drastically affect the run time of the transformed programs. For example,
compare a machine that can add and subtract to one that can only increment
and decrement. Addition and subtraction can easily be implemented in terms of
increment and decrement, but it takes linear time in the size of the numbers,
not constant.

<p>So, say I constrain the space of allowable architectures to those where
software for a reasonably capable (say, 6502-like) processor can be reduced
to the architecture with constant overhead. That is, each source instruction
should be implementable in a constant number of target instructions.

<p>I also need to be able to actually make the thing, and I want to do as
little wiring as possible. So, I'm looking for an architecture similar enough
to a modern one to be constant-time-reducible to it, but one very simple to
implement in hardware.

<h2>One Instruction to Rule Them All</h2>

<p>Academics and nerds have invented a dazzling variety of Turing-complete
computer architectures with only one type of instruction. These are
one-instruction-set computers, or OISCs. An OISC suits my purposes nicely,
since with only one instruction type, I wouldn't need instruction decoding or
switching logic. SUBLEQ (subtract and branch if result <= 0) is probably the
most famous instruction to use for this purpose. Implementing it still seems
difficult though; I'd need an ALU capable of subtraction, complete with
status bits for the branch.

<p>At a bare minimum, the CPU is going to need to be able to fetch data from
and deliver data to main memory. I was surprised to learn that a combination
of these two basic operations, read and write, is also Turing-complete.

<p>The OISC byte-byte-jump consists of an read from a source address, a write
of that value to a target address, and a jump to a destination address. The
instruction in memory consists of just those three addresses, stored
sequentially. Remarkably, this instruction is Turing-complete on
stored-program computers. Even more remarkably, it admits a constant-time
implementation of a modern CPU's instructions. Yet more remarkably, it only
takes around 200 byte-byte-jumps to implement most operations. If I can
clock this thing fast enough, it will resemble an actual CPU, albeit one
about 200 times slower.