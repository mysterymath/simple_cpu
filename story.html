<!doctype html>
<html lang="en-US">
<title>A Simple CPU</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>
    html {
      background-color: #002b36;
    }
    .content {
      color: #839496;
      font-family: Georgia, serif;
      line-height: 1.5;
      max-width: 60ch;
      margin-left: auto;
      margin-right: auto;
    }
    h1,h2,h3,h4,h5,h6 {
      color: #93a1a1;
      font-family: sans-serif;
      line-height: normal;
    }
    p,ul,ol {
      break-inside: avoid;
    }
    a:link {
      color: #268bd2;
    }
    a:visited {
      color: #d33682;
    }
    code {
      font-size: large;
    }
    .not {
      text-decoration: overline;
    }
</style>
<div class="content">
<h1>A Simple CPU</h1>
<h2>Magic</h2>
<p>When I was a child, I marvelled at our family computer. I played a lot of
Duke Nukem 3D and Doom, but every once in a while, I'd get bored of shooting
things. Invariably, I'd lean my face in real close the the monitor and start
nudging the arrow keys. As the game camera moved through the virtual space,
the pixellated rectangles on screen would distort in complex patterns. I was
convinced that if I just stared at it long enough, I'd figure out the trick
they used to make the colors on screen change to look like a 3D room.

<p>In high school, I finally learned enough math to figure it out. I even built
a little ray-casting 3D engine to boot. (Ironically, neither of the above games
used ray-casting.) It was one thing to have figured out on paper how wall
distances could be used to shrink or expand a pixel-wide column of texture; it
was another thing entirely to see a room made of your own math on screen.

<p>In my childhood, I'd been shown a magic trick. Hocus pocus, now you see a
room! Later, I'd learned to perform the trick myself.

<p>In college, I learned of computer architecture, compilers, network stacks,
algorithms and all the trappings of software. I took a digital circuit design
class and made sequential logic circuits on breadboards. A deeper magicks had
been revealed: some core tricks behind computing itself.

<p>I understood how these tricks were done, but I couldn't really do them
myself. I wanted to make a CPU to embody what I'd learned, but every path
towards this goal felt either trivial or impossible.  I could hand wire relays
or transistors into flip flops and half adders, but that would take forever. I
could program it all into a FPGA; too easy. I could hook together a bunch of
pre-built ALUs and registers and the like, but that didn't feel satisfying
either. I put the project on the shelf for a long time.

<h2>Simplicity</h2>

<p>The project popped back into my mind a while ago with a twist. Even a toy
CPU usually has instruction decoding, data path switching, an ALU, a bunch of
stuff. But these toy CPUs are simplified models of real CPUs, and real CPUs
need to be efficient, performant, and practical.

<p>My CPU doesn't need to be any of those things; I just want it to compute the
Fibonacci sequence in a day or so. This opens up an enormous new space of
hilariously impractical, but perhaps simpler, designs. Where should I go
looking for hilariously impractical yet simple ideas? Abstract mathematics, of
course.

<p>By definition, any Turing-complete computer architecture can be reduced to
any other via a software transformation. So long as the transformation doesn't
blow a Fibonacci-computing program completely out of the realm of possibility,
and Turing-complete architecture is fine for my purposes. I also don't want to
do a lot of wiring, so I'm looking for the Turing-complete architecture that is
simplest for me to actually put together.

<h2>One Instruction to Rule Them All</h2>

<p>Academics and nerds have invented a dazzling variety of Turing-complete
machines with only one type of instruction: the appropriately-named
one-instruction-set compusters (OISCs). With only one instruction type, I
wouldn't need to decode the instruction type or switch out of the CPU's
behavior. This potentially saves a lot of logic and wiring.

<p>SUBLEQ (subtract and branch if result &lt;= 0) is probably the most famous
instruction to use for this purpose. Implementing it still seems difficult
though; I'd need an ALU capable of subtraction, complete with status bits for
the branch. The CPU would also switch its operation depending on whether or not
the branch were taken.

<p>At a bare minimum, the CPU is going to need to be able to fetch data from
and deliver data to main memory. From consulting various lists of OISCs, I was
surprised to learn that a combination of these two basic operations, read and
write, is Turing-complete.

<p>The OISC byte-byte-jump has one instruction with three steps. First, read a
byte from a source address. Next write the byte to a destination address.
Finally, jump the instruction pointer to a jump address. The instruction in
memory consists of just those three addresses stored consecutively. Remarkably,
this instruction is Turing-complete on stored-program computers. Even more
remarkably, it admits a constant-time implementation of a modern CPU's
instructions. Tremendously remarkably, it only takes around 200 byte-byte-jumps
and a couple KiB of tables to implement most 8-bit addition and similarly
complex operations. If I can clock this thing fast enough, it will resemble an
actual CPU, albeit one about 200 times slower.
