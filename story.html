<!doctype html>
<html lang="en-US">
<title>A Simple CPU</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>
    html {
      background-color: #002b36;
    }
    .content {
      color: #839496;
      font-family: Georgia, serif;
      line-height: 1.5;
      max-width: 60ch;
      margin-left: auto;
      margin-right: auto;
    }
    h1,h2,h3,h4,h5,h6 {
      color: #93a1a1;
      font-family: sans-serif;
      line-height: normal;
    }
    p,ul,ol {
      break-inside: avoid;
    }
    a:link {
      color: #268bd2;
    }
    a:visited {
      color: #d33682;
    }
    code {
      font-size: large;
    }
    .not {
      text-decoration: overline;
    }
</style>
<div class="content">
<h1>A Simple CPU</h1>
<h2>Magic</h2>
<p>When I was a child, I marvelled at our family computer. I played a lot of
Duke Nukem 3D and Doom, but every once in a while, I'd get bored of shooting
things. Invariably, I'd lean my face in real close the the monitor and start
nudging the arrow keys. As the game camera moved through the virtual space,
the pixellated rectangles on screen would distort in complex patterns. I was
convinced that if I just stared at it long enough, I'd figure out the trick
they used to make the colors on screen change to look like a 3D room.

<p>In high school, I finally learned enough math to figure it out. I even built
a little ray-casting 3D engine to boot. (Ironically, neither of the above games
used ray-casting.) It was one thing to have figured out on paper how wall
distances could be used to shrink or expand a pixel-wide column of texture; it
was another thing entirely to see a room made of your own math on screen.

<p>In my childhood, I'd been shown a magic trick. Hocus pocus, now you see a
room! Later, I'd learned to perform the trick myself.

<p>In college, I learned of computer architecture, compilers, network stacks,
algorithms and all the trappings of software. I took a digital circuit design
class and made sequential logic circuits on breadboards. A deeper magicks had
been revealed: some core tricks behind computing itself.

<p>I understood how these tricks were done, but I couldn't really do them
myself. I wanted to make a CPU to embody what I'd learned, but every path
towards this goal felt either trivial or impossible.  I could hand wire relays
or transistors into flip flops and half adders, but that would take forever. I
could program it all into a FPGA; too easy. I could hook together a bunch of
pre-built ALUs and registers and the like, but that didn't feel satisfying
either. I put the project on the shelf for a long time.

<h2>Simplicity</h2>

<p>The project popped back into my mind a while ago with a twist. Even a toy
CPU usually has instruction decoding, data path switching, an ALU, a bunch of
stuff. But these toy CPUs are simplified models of real CPUs, and real CPUs
need to be efficient, performant, and practical.

<p>My CPU wouldn't need to be any of those things; I'd just want it to
compute the Fibonacci sequence in under a day or so. This opened up an
enormous new space of hilariously impractical, but maybe simpler, designs.

<p>Hilarious impracticality and simplicity are both calling cards of abstract
mathematics. By definition, any Turing-complete computer architecture can be
reduced to any other via software rewriting. So long as the transformation
doesn't blow a Fibonacci-computing program completely out of the realm of
feasibility, any Turing-complete architecture would be fine for my purposes.
I also didn't want to do a lot of wiring, so I was looking for an
architecture that is very simple to actually assemble out of electronic
parts.

<h2>One Instruction to Rule Them All</h2>

<p>Academics and nerds have invented a dazzling variety of Turing-complete
machines with only one type of instruction: the appropriately-named
one-instruction-set computers (OISCs). With only one instruction type, I
could skip the whole issue of instruction decoding and associated control
logic.

<p>SUBLEQ (subtract and branch if result &lt;= 0) is probably the most famous
instruction to use for this purpose. I explored this thoroughly, but
implementing it seemed hardly any easier than a more featureful CPU. I'd
still need an ALU capable of subtraction, complete with status bits for the
branch. The CPU would also still have to switch its behavior depending on
whether or not the branch should be taken.

<p>Any CPU needs to fetch data from and deliver data to main memory. From
consulting various lists of OISCs, I was surprised to learn that a
combination of these two basic operations, read and write, is
Turing-complete.

<p>The OISC byte-byte-jump has one instruction with three steps. First, read
a byte from a source address. Next write the byte to a destination address.
Finally, jump the instruction pointer to a jump address. Remarkably, this
instruction is Turing-complete on stored-program computers.

<h2>Can't Add, Doesn't Even Try</h2>

<p>It might seem magical that such a simple operation could implement
arithmetic, comparison, and conditional branching. As with all magic tricks,
it's a bit less impressive once you know how.

<p>Tables. A program can perform any arithmetic operation by including a
boring table mapping all possible inputs to the corresponding output. With
8-bit operations, unary operations are 256-byte  tables, and binary operations
are 2D 64 KiB tables.

<p>Admittedly this felt a bit like cheating, but programs for real
architectures include all sorts of lookup tables for DES S-boxes, hash
functions, and the like. This is no different, except that usually lookup
tables make a program more efficient; here they make a program
<i>possible</i>.

<p>I couldn't actually include a bunch of 64 KiB tables in my program,
though. For aesthetic reasons, I wanted an 8-bit byte, 16-bit address
computer. When I start measuring things in megabytes, the nostalgia goggles
fall right off.

<p>Luckily, there's no need to do an entire 8-bit operation in one go. Two
sequential 4-bit additions do an 8-bit addition just fine, and a 4-bit
addition table takes only 256 bytes. I did need a little bit of cleverness to
merge the high and low 4 bits of two different bytes together, but it's not
very interesting, and it only cost another 4KiB of tables.

<p>Speaking of lookups, I haven't mentioned yet how byte-byte-jump can
perform a table lookup at all. This trick is an old one: indexed lookup by
self-modifying code. Write a byte-byte-jump instruction into the program, and
set the source address to the beginning of a 256-byte-aligned table. Precede
this with an instruction that overwrites the low byte of the source address
with the table index. When the later instruction executes, it will perform a
table lookup. Overwriting the jump address in this fashion instead does
conditional branching.

<p>The cheapest configuration of the <a
href="https://en.wikipedia.org/wiki/IBM_1620">IBM 1620</a> used this kind of
arithmetic table to replace expensive logic with cheaper program storage
(punched cards and the like). It's internal code name, CADET, has been said
to stand for "Can't Add, Doesn't Even Try." I rather like that.