<!doctype html>
<html lang="en-US">
<title>A Simple CPU</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>
    html {
      background-color: #002b36;
    }
    .content {
      color: #839496;
      font-family: Georgia, serif;
      line-height: 1.5;
      max-width: 60ch;
      margin-left: auto;
      margin-right: auto;
    }
    h1,h2,h3,h4,h5,h6 {
      color: #93a1a1;
      font-family: sans-serif;
      line-height: normal;
    }
    h1 {
      text-align: center;
    }
    h2 {
      font-size: 1.2em;
    }
    p,ul,ol {
      break-inside: avoid;
    }
    a:link {
      color: #268bd2;
    }
    a:visited {
      color: #d33682;
    }
    code {
      font-size: large;
    }
    .not {
      text-decoration: overline;
    }
    .video {
      max-width: 100%;
      /* This is relative to the width, preserving the aspect ratio. */
      padding-bottom: 56.25%;
      position: relative;
      width: 560px;
    }
    iframe {
      height: 100%;
      left: 0;
      position: absolute;
      top: 0;
      width: 100%;
    }
</style>
<div class="content">
<h1>A Simple CPU</h1>
<h2>Magic</h2>
<p>When I was a child, I marvelled at our family computer. I played a lot of
Duke Nukem 3D and Doom, but every once in a while, I'd get bored of shooting
things. Invariably, I'd lean my face in real close the the monitor and start
nudging the arrow keys. As the game camera moved through the virtual space,
the pixellated rectangles on screen would distort in complex patterns. I was
convinced that if I just stared at it long enough, I'd figure out the trick
they used to make the colors on screen change to look like a 3D room.

<p>In high school, I finally learned enough math to figure it out. I even built
a little ray-casting 3D engine. (Although I later learned neither game actually
used ray-casting. Que sera.) It was one thing to have figured out on paper how
wall distances could be used to shrink or expand a pixel-wide column of
texture; it was another thing to walk around a room made of your own math.

<p>In my childhood, I'd been shown a magic trick. Hocus pocus, now you see a
room! Later, I'd learned the trick myself.

<p>In college, I learned of computer architecture, compilers, network stacks,
algorithms and all the trappings of software. I took a digital circuit design
class and made sequential logic circuits on breadboards. A deeper magicks had
been revealed: the core of computing itself.

<p>I understood how these tricks were done, but I couldn't really do them
myself. I wanted to make a CPU to embody what I'd learned, but every path
towards this goal felt either trivial or impossible.  I could hand wire relays
or transistors into flip flops and half adders, but that would take forever. I
could program it all into a FPGA; too easy. I could hook together a bunch of
pre-built ALUs and registers and the like, but that felt like assembling LEGOs.
So I put the project on the shelf for a long time.

<h2>Simplicity</h2>

<p>The project popped back into my mind a while ago with a twist. Even a toy
CPU usually has instruction decoding, data path switching, an ALU, a bunch of
stuff. But these toy CPUs are simplified models of real CPUs, and real CPUs
need to be efficient, performant, and practical.

<p>My CPU wouldn't need to be any of those things; I'd just want it to
compute the Fibonacci sequence in under a day or so. This opened up an
enormous new space of hilariously impractical, but maybe simpler, designs.

<p>Hilarious impracticality and simplicity are both calling cards of abstract
mathematics. Any <a href="https://en.wikipedia.org/wiki/Turing_completeness">Turing-complete</a>
computer architecture can be reduced to any other via software rewriting. So
long as the transformation doesn't make Fibonacci program completely
infeasibile, any of these architecture would work just fine.  I also
didn't want to do a lot of wiring, so I was also looking for one that could be
very simply constructed from electronic parts.

<h2>One instruction to rule them all</h2>

<p>Academics and nerds have invented a dazzling variety of Turing-complete
machines with only one type of instruction. These are appropriately named <a
href="https://en.wikipedia.org/wiki/One_instruction_set_computer">one
instruction set computers</a> (OISCs). With only one instruction type, I could
skip the whole issue of instruction decoding and associated control logic.

<p><a href="https://en.wikipedia.org/wiki/One_instruction_set_computer#Subtract_and_branch_if_less_than_or_equal_to_zero">SUBLEQ</a>
(subtract and branch if result &lt;= 0) is probably the most famous instruction
to use for this purpose. I explored this thoroughly, but implementing it seemed
hardly any easier than a more featureful CPU. I'd still need an ALU capable of
subtraction, complete with status bits for the branch. The CPU would also still
have to switch its behavior depending on whether or not the branch should be
taken.

<p>Any CPU needs to fetch data from and deliver data to main memory. From
consulting <a href="https://en.wikipedia.org/wiki/One_instruction_set_computer">various</a>
<a href="https://esolangs.org/wiki/OISC">lists</a> of OISCs, I was surprised to learn that a
combination of these two basic operations, read and write, is Turing-complete.

<p>The OISC instruction
<a href="https://esolangs.org/wiki/ByteByteJump">byte-byte-jump</a>
takes three addresses as arguments: source, destination, and jump. First, it
reads byte from the source address. Next, it writes thar byte to the
destination address. Finally, it changes the instruction pointer to the jump
address. Remarkably, this instruction is Turing-complete on
<a href="https://en.wikipedia.org/wiki/Stored-program_computer">stored-program computers</a>.

<h2>Can't add, doesn't even try</h2>

<p>It might seem magical that such a simple operation could implement
arithmetic, comparison, and conditional branching. As with all magic tricks,
it's a bit less impressive once you know how.

<p><a href="https://en.wikipedia.org/wiki/Lookup_table">Tables</a>.
A program can perform any arithmetic operation by including a boring table
mapping all possible inputs to the corresponding output. With 8-bit operations,
unary operations are 256-byte  tables, and binary operations are 2D 64 KiB
tables.

<p>Admittedly this felt a bit like cheating, but programs for real
architectures include all sorts of lookup tables for DES S-boxes, hash
functions, and the like. This is no different, except that usually lookup
tables make a program more efficient; here they make a program
<i>possible</i>.

<p>I couldn't actually include a bunch of 64 KiB tables in my program, though.
For aesthetic reasons, I wanted an 8-bit byte, 16-bit address computer. When I
start measuring things in megabytes, the nostalgia goggles fall right off.

<p>Luckily, there's no need to do an entire 8-bit operation in one go. Two
sequential 4-bit additions do an 8-bit addition just fine, and a 4-bit
addition table takes only 256 bytes. I did need a little bit of cleverness to
merge the high and low 4 bits of two different bytes together, but it's not
very interesting, and it only cost another 4 KiB of tables.

<p>Speaking of lookups, I haven't mentioned yet how byte-byte-jump can
perform a table lookup at all. This trick is an old one: indexed lookup by
self-modifying code. Write a byte-byte-jump instruction into the program, and
set the source address to the beginning of a 256-byte-aligned table. Precede
this with an instruction that overwrites the low byte of the source address
with the table index. When the later instruction executes, it will perform a
table lookup. Overwriting the jump address in this fashion instead does
conditional branching.

<p>The cheapest configuration of the <a
href="https://en.wikipedia.org/wiki/IBM_1620">IBM 1620</a> used these kinds of
arithmetic tables to replace expensive logic with cheaper program storage
(punched cards and the like). It's internal code name, CADET, has been said to
stand for "Can't Add, Doesn't Even Try." I rather like that.

<h2>Registers</h2>

<p>With software out of the way, let's talk about hardware. My byte-byte-jump
CPU needs to do four steps in an infinite loop:

<ol>
  <li>Read the instruction at the instruction pointer.
  <li>Read the byte at the source address.
  <li>Write the byte to the destination address.
  <li>Change the instruction pointer to the jump address.
</ol>

<p>Steps 2-4 require remembering the instrucion read in step 1, and step 3
requires remembering the value read in step 2. Thus, the CPU will need to
remember 24 bits or so of data over time. Let's draw a box around whatever does
this and call it a "memory module."

<p>When communicating with a memory module, the different bits of the address
and data need to be distinguished from one another somehow. There are two main
hardware mechanisms for doing so: parallel and serial communications.  Parallel
distinguishes bits using space, and serial using time.  Parallel gives each bit
their own pin, while serial sends them all through the same pin, using some
protocol.  Serial is the obvious winner in terms of build complexity.

<p>I couldn't find any small serial register chips, so I started shopping for
serial SRAMs. Not that many companies make cheap serial SRAMs, so it wasn't too
hard to narrow this down to the 64KiB <a
href="https://www.microchip.com/wwwproducts/en/23LC512">Microchip 23LC512</a>.
This uses <a href="https://en.wikipedia.org/wiki/Serial_Peripheral_Interface">SPI</a>
to communicate data, addresses, and read/write operations to and from the chip.
SPI is just about the simplest serial communications protocol imaginable, and
the chip only costs around a dollar. I had my CPU registers.

<p>I'll pause for a moment to revel in the absurdity of using a 64KiB SRAM to
remember a couple bits worth of information. This one chip has more switching
elements than the first ten of mankinds' computers combined. I'd have used a
smaller one, but none of the smaller SPI SRAMs I could find defaulted to
"sequential mode," which I needed. To use them, I'd have needed dedicated
logic to switch their mode on boot. Even then, smallest available was still
8KiB, so whatever.

<p>At this point I became concerned that the SRAM I had just purchased was
basically a modified CPU, which would make the whole thing cheating. After all,
this memory chip has its own internal instruction pointer, instruction set,
communications channel in and out of the chip, and (of course) internal memory.
But its instruction set is only read, write, and mode setting, and all the
modes do is slightly modify how reads and writes work. Unless I'm mistaken, the
chip isn't Turing-complete, even if it has some of the trappings of it. Really,
if I want to build a CPU as easily as possible, I'd want the parts I'm using to
be almost, but not quite, Turing complete. I can just do the fun bit of
sprinkling it with Turing pixie-dust. Shoulders of giants and so forth.

<h2>Control</h2>

<p>So, the SRAM chip has only 8 pins, which makes sense, since it's serial.
But 8 still seems like a lot for serial communications, so let's unpack them a
bit. There's power, ground, clock, and two not used for our purposes; that
leaves 3 serial pins. These are chip select, serial input, and serial output.
The chip isn't actually capable of inputing and outputting data at the same
time, so at any given time one is completely disconnected. I wired the two
together and called the combination SIO. This leaves 2: SIO and chip select.
Chip select basically tells the chip where the boundary is between one
read/write and another.

<p>Two pins to drive. Well, the serial input/output pin, call it SIO, I can
connect up to "the rest of the system." This allows the CPU to communicate with
"main memory" and "I/O devices". In my case, that's a Raspberry PI's GPIO pins.
I added another pin to act as chip select for the "system", and used a simplified
version of the SRAM's SPI read/write protocol to drive it.

<p>So, three signals to drive: SIO, SRAM chip select, and system chip select.
Some combination of 1's and 0's at various times on these four pins, and the
SRAM will execute byte-byte-jump. Begin a read on the system, have the SRAM
send the instruction pointer to the system, have the SRAM write the instruction
to address 0, implicitly overwriting the instruction pointer, and so on. The
actual derivation of these signals was extremely laborious, and took me a few
months of fiddling in my off-hours.

<p>So, how to drive these pins... how indeed. More memory chips! In this case,
the 1024-bit <a href="http://www.microchip.com/wwwproducts/25LC010A">Microchip
25LC010A</a> SPI EEPROM chips, which also cost about a dollar.  I can generate
basically any signal I want by filling a SPI EEPROM chip with it and issuing a
"sequential mode" read to address zero. Since the chip's internal address
pointer loops around, the chip will clock one bit at a time out of the ROM
indefinitely. Three EEPROMs can drive the lines to whatever bit pattern I
desire, just so long as it can be made to fit evenly some number of times in
1024 bits.

<p>One caveat: SIO? At some points in the cycle, the SRAM drives it, say, to
send the system the instruction pointer. At other times, the system drives it,
say, to send the instruction back. At yet other times, control logic drives it,
say, to tell the SRAM where in its memory to save the instruction. The SRAM and
system already have mechanisms for "not speaking" built into SPI; a
disconnected/high-impedance state (Z) is part of the protocol. But the EEPROM
driving SIO also needs a third, Z state as well.

<p>Luckily, these EEPROM chips provide a hold pin that Z's the EEPROMs output.
How do we drive the hold pin? Yet another EEPROM, of course, for a total of four.

<h2>Implementation</h2>

<p>This puts the total bill of materials at:

<ul>
  <li>Microchip SRAM
  <li>4 Microchip EEPROMs
  <li>2 pullup resistors (for hold and system chip select)
  <li>4 LEDs w/ resistors, to provide blinkenlights for each signal
  <li>40 or so wires
</ul>

<p>I bought all this from Mouser and waited for it to arrive. It cost me about
$14, and $7 of that was shipping.

<p>I wrote a RPi.GPIO program to accept read and write operations over the SIO
and system chip select lines, using a 64 KiB Python array array as backing
storage. I special-cased writes to 0xFFFF to print to the terminal. Believe it
or not, it executed byte-byte-jumps the very first time I turned it on.

<p>I then incrementally built up a library of macros using Python to program a
Fibonacci generating program into the memory array. It took around 200 clock
cycles to execute a byte-byte-jump, and around 200 byte-byte-jumps to exectue
an 8-bit addition.

<p>You can see it in action at various clock rates below. The second-rightmost
LED is SIO, and all of the others are control lines. All the Turing-magic
happens on that one LED; the others are perfectly cyclical, since they are
fully driven by EEPROMs. The SIO line is only driven by EEPROM when the
rightmost (HOLD) LED is on.

<div class="video">
  <iframe
    src="https://www.youtube-nocookie.com/embed/videoseries?list=PL4Aym2GNCg4hb7xvSkjSgP3m2xWIY_TZK"
    frameborder="0"
    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>
</div>

<h2>Discussion</h2>

<p>Compared to the time it took to conceive, design, and program this CPU,
building it was incredibly straightforward. It operated reliably at all the
clock rates I could get Python to generate. Admittedly, that's not very fast,
but I got the thing running at 80kHz, and it's only takes around 40,000 cycles
to do an addition. That is, it can do around 2 additions per second, which is
more than I can do, and I bet it can do them all night!

<p>I can't say I've learned very much about how to make a real CPU, but I did
learn a lot about avoiding bus contention, SIO, timing analysis, and general
hardware design. I was also surprised that the final design seems to have far
more in common with early computers than modern ones. Early computers were all
bit-serial like this one, and very limited instruction sets supplemented with
lookup tables were common. When you think about it, that's not too surprising,
since in both cases simple implementation trumps most concerns.  other
criterion. I wanted it to be simple because I'm lazy; but they needed it to be
simple, since technology placed tight limitations on what they could build,
even with the backing of major world governments and universities.

<p>I can safely say that running this CPU for the first time was exactly as
satisfying as I'd hoped.  I still can't believe the damn thing actually
<b>computes</b>. Given how many things could have broken along the way, the
fact that it does what I wanted is, well, pretty magical.

<p>I left out a tremendous amount of detail about how the CPU works, because it
was extremely boring. Still, if any reader has specific questions about how
how this works, shoot me an email.

<address>Daniel Thornburgh (<a href="mailto:mysterymath@gmail.com">mysterymath@gmail.com</a>)</address>
