<!doctype html>
<html lang="en-US">
<title>A Simple CPU</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<style>
    html {
      background-color: #002b36;
    }
    .content {
      color: #839496;
      font-family: Georgia, serif;
      line-height: 1.5;
      max-width: 60ch;
      margin-left: auto;
      margin-right: auto;
    }
    h1,h2,h3,h4,h5,h6 {
      color: #93a1a1;
      font-family: sans-serif;
      line-height: normal;
    }
    h1 {
      text-align: center;
    }
    h2 {
      font-size: 1.2em;
    }
    p,ul,ol {
      break-inside: avoid;
    }
    a:link {
      color: #268bd2;
    }
    a:visited {
      color: #d33682;
    }
    code {
      font-size: large;
    }
    .not {
      text-decoration: overline;
    }
    .video {
      max-width: 100%;
      /* This is relative to the width, preserving the aspect ratio. */
      padding-bottom: 56.25%;
      position: relative;
      width: 560px;
    }
    iframe {
      height: 100%;
      left: 0;
      position: absolute;
      top: 0;
      width: 100%;
    }
</style>
<div class="content">
<h1>A Simple CPU</h1>
<h2>Magic</h2>
<p>When I was a child, I marvelled at our family computer. I played a lot of
Duke Nukem 3D and Doom, but every once in a while, I'd get bored of shooting
things. Invariably, I'd lean my face in real close the the monitor and start
nudging the arrow keys. As the game camera moved through the virtual space,
the pixellated rectangles on screen would distort in complex patterns. I was
convinced that if I just stared at it long enough, I'd figure out the trick
they used to make the colors on screen change to look like a 3D room.

<p>In high school, I finally learned enough math to figure it out. I even built
a little ray-casting 3D engine to boot. (Ironically, neither of the above games
used ray-casting.) It was one thing to have figured out on paper how wall
distances could be used to shrink or expand a pixel-wide column of texture; it
was another thing entirely to see a room made of your own math on screen.

<p>In my childhood, I'd been shown a magic trick. Hocus pocus, now you see a
room! Later, I'd learned to perform the trick myself.

<p>In college, I learned of computer architecture, compilers, network stacks,
algorithms and all the trappings of software. I took a digital circuit design
class and made sequential logic circuits on breadboards. A deeper magicks had
been revealed: some core tricks behind computing itself.

<p>I understood how these tricks were done, but I couldn't really do them
myself. I wanted to make a CPU to embody what I'd learned, but every path
towards this goal felt either trivial or impossible.  I could hand wire relays
or transistors into flip flops and half adders, but that would take forever. I
could program it all into a FPGA; too easy. I could hook together a bunch of
pre-built ALUs and registers and the like, but that didn't feel satisfying
either. I put the project on the shelf for a long time.

<h2>Simplicity</h2>

<p>The project popped back into my mind a while ago with a twist. Even a toy
CPU usually has instruction decoding, data path switching, an ALU, a bunch of
stuff. But these toy CPUs are simplified models of real CPUs, and real CPUs
need to be efficient, performant, and practical.

<p>My CPU wouldn't need to be any of those things; I'd just want it to
compute the Fibonacci sequence in under a day or so. This opened up an
enormous new space of hilariously impractical, but maybe simpler, designs.

<p>Hilarious impracticality and simplicity are both calling cards of abstract
mathematics. By definition, any Turing-complete computer architecture can be
reduced to any other via software rewriting. So long as the transformation
doesn't blow a Fibonacci-computing program completely out of the realm of
feasibility, any Turing-complete architecture would be fine for my purposes.
I also didn't want to do a lot of wiring, so I was looking for an
architecture that is very simple to actually assemble out of electronic
parts.

<h2>One instruction to rule them all</h2>

<p>Academics and nerds have invented a dazzling variety of Turing-complete
machines with only one type of instruction: the appropriately-named
one-instruction-set computers (OISCs). With only one instruction type, I
could skip the whole issue of instruction decoding and associated control
logic.

<p>SUBLEQ (subtract and branch if result &lt;= 0) is probably the most famous
instruction to use for this purpose. I explored this thoroughly, but
implementing it seemed hardly any easier than a more featureful CPU. I'd
still need an ALU capable of subtraction, complete with status bits for the
branch. The CPU would also still have to switch its behavior depending on
whether or not the branch should be taken.

<p>Any CPU needs to fetch data from and deliver data to main memory. From
consulting various lists of OISCs, I was surprised to learn that a
combination of these two basic operations, read and write, is
Turing-complete.

<p>The OISC byte-byte-jump has one instruction with three steps. First, read
a byte from a source address. Next write the byte to a destination address.
Finally, jump the instruction pointer to a jump address. Remarkably, this
instruction is Turing-complete on stored-program computers.

<h2>Can't add, doesn't even try</h2>

<p>It might seem magical that such a simple operation could implement
arithmetic, comparison, and conditional branching. As with all magic tricks,
it's a bit less impressive once you know how.

<p>Tables. A program can perform any arithmetic operation by including a
boring table mapping all possible inputs to the corresponding output. With
8-bit operations, unary operations are 256-byte  tables, and binary operations
are 2D 64 KiB tables.

<p>Admittedly this felt a bit like cheating, but programs for real
architectures include all sorts of lookup tables for DES S-boxes, hash
functions, and the like. This is no different, except that usually lookup
tables make a program more efficient; here they make a program
<i>possible</i>.

<p>I couldn't actually include a bunch of 64 KiB tables in my program,
though. For aesthetic reasons, I wanted an 8-bit byte, 16-bit address
computer. When I start measuring things in megabytes, the nostalgia goggles
fall right off.

<p>Luckily, there's no need to do an entire 8-bit operation in one go. Two
sequential 4-bit additions do an 8-bit addition just fine, and a 4-bit
addition table takes only 256 bytes. I did need a little bit of cleverness to
merge the high and low 4 bits of two different bytes together, but it's not
very interesting, and it only cost another 4KiB of tables.

<p>Speaking of lookups, I haven't mentioned yet how byte-byte-jump can
perform a table lookup at all. This trick is an old one: indexed lookup by
self-modifying code. Write a byte-byte-jump instruction into the program, and
set the source address to the beginning of a 256-byte-aligned table. Precede
this with an instruction that overwrites the low byte of the source address
with the table index. When the later instruction executes, it will perform a
table lookup. Overwriting the jump address in this fashion instead does
conditional branching.

<p>The cheapest configuration of the <a
href="https://en.wikipedia.org/wiki/IBM_1620">IBM 1620</a> used this kind of
arithmetic table to replace expensive logic with cheaper program storage
(punched cards and the like). It's internal code name, CADET, has been said
to stand for "Can't Add, Doesn't Even Try." I rather like that.

<h2>Registers</h2>

<p>With software out of the way, let's talk about hardware. The CPU needs to
do four steps in an infinite loop:

<ol>
  <li>Read (and remember) the instruction at the instruction pointer.
  <li>Read (and remember) the byte at the source address.
  <li>Write the byte to the destination address.
  <li>Change the instruction pointer to the jump address.
</ol>

<p>Essentially, the CPU needs to remember a couple bits of data, so it needs
memory. There are two ways to communicate data and addresses with a memory
module: parallel and serial. Parallel would involve between 16 and 24 pins to
wire up. Serial would involve roughly one pin. Easy choice.

<p>I couldn't find any small serial register chips, so I started shopping for
serial SRAMs, selecting the 64KiB Microchip 23LC512. This uses SPI to
communicate data, addresses, and read/write operations to and from the chip.
SPI is just about the simplest serial communications protocol imaginable, and
the chip only costs around a dollar. I had my CPU registers.

<p>I'll pause for a moment to revel in the absurdity of using a 64KiB SRAM to
remember a couple bits worth of information. This one chip has more switching
elements than the first ten of mankinds' computers combined. I'd have used a
smaller one, but none of the smaller SPI SRAMs I could find defaulted to
"sequential mode," which I needed. To use them, I'd have needed dedicated
logic to switch their mode on boot. Even then, smallest available was still
8KiB, so whatever.

<p>At this point I became concerned that the SRAM I had just purchased was
basically a modified CPU, which would make the whole thing cheating. After
all, this memory chip has its own internal instruction pointer, instruction
set, communications channel in and out of the chip, and (of course) internal
memory. But its instruction set is only read, write, and mode setting, and
all the modes do is slightly modify how reads and writes work. Unless I'm
mistaken, the chip itself is pretty far from Turing-complete; it's just a
memory. Really, if I want to build a CPU as easily as possible, it's good
that the SRAM I'm using has all that stuff; it saves me having to build it. I
can just do the fun bit of sprinkling it with Turing pixie-dust. Shoulders of
giants and so forth.

<h2>Control</h2>

<p>So, the SRAM chip has only 8 pins, which makes sense, since it's serial.
But 8 still seems like a lot for serial communications, so let's unpack this
a bit. There's power, ground, clock, and two not used for our purposes; that
leaves 3 data pins. These are chip select, serial input, and serial output.
The chip isn't actually capable of inputing and outputting data at the same
time, so we can wire the input and output together; this leaves just 2. In
SPI, chip select basically tells the chip when an operation begins and ends;
in between, instructions, data, and addresses are clocked in and out of the
chip.

<p>Two pins to drive. Well, the serial input/output pin, call it SIO, we
can also connect up to "the rest of the system." In my case, I used a
Raspberry Pi's GPIO pins. Actually, let's add another pin; we can give the
"rest of the system" a chip select line and drive it over SPI too. I
basically cribbed the read/write protocol from the SRAM and used it for the
Raspberry Pi. This allows the Pi to pretend to be all of the devices the CPU
would be connected to; main memory, terminal input and output, etc.

<p>So, three signals: SIO, SRAM chip select, and system chip select.
Some combination of 1's and 0's at various times on these four pins, and the
SRAM will execute byte-byte-jump. Begin a read on the system, have the SRAM
send the instruction pointer to the system, have the SRAM write the
instruction to address 0, implicitly overwriting the instruction pointer, and
so on. The actual derivation of these signals was extremely laborious, and took
me a few months of fiddling in my off-hours.

<p>More memories! In this case, 1KiB SPI ROM chips, from the same
manufacturer, at about the same price. It turns out I can generate basically
any signal I want by filling a SPI EEPROM chip with it, then issuing a
"sequential mode" read to address zero. The internal address pointer loops
around, so it will continue emitting that sequence of bits indefinitely. I
can just buy three EEPROMs, and they'll drive the lines to whatever bit
pattern I desire.

<p>One caveat: what drives SIO? At some points in the cycle, the SRAM drives
it, say, to send the system the instruction pointer. At other times, the
system drives it, say, to send the instruction back. At yet other times,
control logic drives it, say, to tell the SRAM where in its memory to save
the instruction. The SRAM and system already have mechanisms for "not
speaking" built into SPI; a disconnected/high-impedance state is part of the
protocol. But the signal driving SIO needs three states; not two. On, off,
and "shhhh".

<p>Luckily, these EEPROM chips provide a hold pin that shushes the EEPROMs
output. How do we drive it? Yet another EEPROM, of course, for a total of four.

<h2>Implementation</h2>

<p>This puts the total bill of materials at:

<ul>
  <li>1x Raspberry Pi
  <li>1x Microchip SRAM
  <li>4x Microchip EEPROM
  <li>2 pullup resistors (for hold and system chip select)
  <li>4x LED and resistors, to provide blinkenlights for each signal
  <li>40 or so wires
</ul>

<p>I reduced a Fibonacci calculating program to byte-byte-jump, then prepared
a 64KiB Python array with the program. I wrote a RPi.GPIO program to accept
system read and system write instructions from the CPU, then to access or
modify this array accordingly. I special-cased writes to 0xFFFF to print to
the terminal. Believe it or not, it worked the first time I turned it on.

<p>You can see it in action below, running at various clock rates. The
second-rightmost LED is SIO, and all of the others are control lines. All the
Turing-magic happens on that one LED; the others are perfectly cyclical.

<div class="video">
  <iframe
    src="https://www.youtube-nocookie.com/embed/videoseries?list=PL4Aym2GNCg4hb7xvSkjSgP3m2xWIY_TZK"
    frameborder="0"
    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen></iframe>
</div>

<h2>Discussion</h2>

<p>Compared to the time it took to conceive, design, and program this CPU,
building it was incredibly straightforward. It operated reliably at all the
clock rates I could get Python to generate. Admittedly, that's not very fast,
but I got the thing running at 80kHz, and it's only takes around 40,000
cycles to do an addition. That is, it can do ~2 additions per second, which is
considerably more than I can do, and I bet it can do them all night!

<p>I can't say I've learned very much about how to make a real CPU, but I did
learn a lot about bus contention, SIO, timing analysis, and various other
hardware design issues. I was also surprised that the final design seems to
have far more in common with early computers than modern ones. All modern
computers were bit-serial, as was this one, and extremely limited instruction
sets were common. When you think about it, that's not too surprising, since
in both cases the goal is simple implementation over essentially every other
criterion. Mine was borne out of a peculiar sort of laziness, but theirs out
of necessity and limited technology.

<p>My first draft of this writup included a truly insane amount of detail
about how the CPU works. After getting ready to post it, I gave it one final read-through,
and I couldn't get a third of the way through it without getting too bored and having to stop.
I decided this was a bad sign. Still, some of you may have more specific questions about how this works,
perhaps to build something similar yourself. In that case, shoot me an Email.

<p>Daniel Thornburgh
<p>mysterymath@gmail.com
